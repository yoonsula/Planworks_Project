{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T01:41:56.429081Z",
     "start_time": "2022-08-24T01:41:56.418844Z"
    }
   },
   "source": [
    "<p style =\"font-size:30px; font-weight:bold; color:green\"> 데이터청년캠퍼스 플랜웍스팀 파이썬 코드</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T02:42:43.451692Z",
     "start_time": "2022-08-24T02:42:43.441703Z"
    }
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google map Crawling code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium version setting\n",
    "# !pip install --upgrade selenium==3.141.0\n",
    "\n",
    "# import package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By  # 3개는 selenium이 동작하면서 웹 화면이 로딩될때 까지 기다림을 지원\n",
    "from selenium.webdriver.support.ui import WebDriverWait # 웹 드라이버 미리 설치\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in chromedriver\n",
    "driver = webdriver.Chrome('C:/Users/chromedriver.exe')\n",
    "# enter review url\n",
    "url = 'https://www.google.com/maps/place/%ED%9B%84%EC%A7%84%ED%95%AD/@38.132737,128.6213,17z/data=!4m11!1m2!2m1!1z7ZuE7KeE7ZWt!3m7!1s0x5fd8aff6e709b405:0x7b04ed680c3c22f4!8m2!3d38.132886!4d128.6226878!9m1!1b1!15sCgntm4Tsp4Ttla2SAQZtYXJpbmE?hl=en'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "# sort to most relevant review \n",
    "wait = WebDriverWait(driver, 10)\n",
    "menu_bt = wait.until(EC.element_to_be_clickable(\n",
    "                       (By.XPATH, '//button[@data-value=\\'Sort\\']'))\n",
    "                   )  \n",
    "menu_bt.click()\n",
    "\n",
    "recent_rating_bt = driver.find_elements_by_xpath(\n",
    "                                     '//li[@role=\\'menuitemradio\\']')[0]\n",
    "recent_rating_bt.click()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# scroll code\n",
    "# total n review (10 reviews per 1 page)\n",
    "from tqdm import tqdm\n",
    "\n",
    "page_num = 50\n",
    "per = 10\n",
    "pbar = tqdm(total=page_num*per)\n",
    "with tqdm(total=page_num*per, position=0, leave=True) as pbar:\n",
    "    for i in range(page_num):\n",
    "        buffer = driver.find_elements_by_class_name('qjESne') # buffer location\n",
    "        action = ActionChains(driver)\n",
    "\n",
    "        n =len(buffer)\n",
    "        action.move_to_element(buffer[n-1]).perform() # move to last buffer\n",
    "        time.sleep(1)\n",
    "        pbar.update(per)\n",
    "    pbar.close()\n",
    "\n",
    "# review item list    \n",
    "box_list = []\n",
    "name_list = []\n",
    "stars_list = []\n",
    "review_list = []\n",
    "date_list = []\n",
    "\n",
    "boxes = driver.find_elements_by_class_name('jftiEf.fontBodyMedium') # review box class\n",
    "\n",
    "# more butten click\n",
    "for box in boxes:\n",
    "    try:\n",
    "        more_btn = box.find_element_by_class_name('w8nwRe.kyuRq')\n",
    "        more_btn.click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "# review collection\n",
    "for box in boxes:\n",
    "    # each class location\n",
    "    IDs = box.find_elements_by_class_name('d4r55')\n",
    "    Dates = box.find_elements_by_class_name('rsqaWe')\n",
    "    Stars = box.find_elements_by_class_name('kvMYJc')\n",
    "    reviews = box.find_elements_by_css_selector('span.wiI7pd')\n",
    "\n",
    "    # append to list after each review collection\n",
    "    for ID, Date, Star, review in zip(IDs, Dates, Stars, reviews):\n",
    "        name_list.append(ID.text)\n",
    "        date_list.append(Date.text)\n",
    "        stars_list.append(Star.get_attribute('aria-label'))\n",
    "        review_list.append(review.text)\n",
    "        \n",
    "# review list to dataframe\n",
    "review = pd.DataFrame(\n",
    "    {'name': name_list,\n",
    "     'rating': stars_list,\n",
    "     'review': review_list,\n",
    "     'date': date_list\n",
    "    })\n",
    "\n",
    "# check number of review\n",
    "print(len(review))\n",
    "print(review.tail())\n",
    "\n",
    "\n",
    "# We use only foreign tourist. So deletes data with Korean in names and reviews\n",
    "\n",
    "eng_review = review.copy()\n",
    "\n",
    "# remove korean name\n",
    "eng_review['name2'] = eng_review['name'].str.findall(r'[ㄱ-ㅎㅏ-ㅣ가-힣]')\n",
    "eng_review = eng_review[eng_review['name2'].str.len() == 0]\n",
    "\n",
    "# remove korean review\n",
    "eng_review['review2'] = eng_review['review'].str.findall(r'[ㄱ-ㅎㅏ-ㅣ가-힣]')\n",
    "eng_review = eng_review[eng_review['review2'].str.len() == 0]\n",
    "\n",
    "# remove name2 & review2 columns\n",
    "eng_review.drop(['name2','review2'],axis=1, inplace=True)\n",
    "\n",
    "# remove \"(Translated by Google)\" sentence\n",
    "eng_review['review'] = eng_review['review'].str.replace('(Translated by Google) ','',regex=False)\n",
    "\n",
    "# remove \"Original\" word\n",
    "eng_review['review'] = eng_review['review'].str.replace('\\(Original\\)','',regex=False)\n",
    "\n",
    "# remove not english sentence & special characters\n",
    "eng_review['review'] = eng_review['review'].str.replace(r'[^A-Za-z0-9]',' ', regex=True)\n",
    "\n",
    "# delete side blank and \"\\n\"\n",
    "eng_review['review'] = eng_review['review'].str.strip()\n",
    "\n",
    "# replace with lowercase\n",
    "eng_review['review'] = eng_review['review'].str.lower()\n",
    "\n",
    "# remove \"orginal\" word\n",
    "eng_review['review'] = eng_review['review'].str.replace('original','')\n",
    "\n",
    "# remove unreviewed data\n",
    "eng_review = eng_review[eng_review['review'].str.len()!=0]\n",
    "\n",
    "print(\"number of review:\", len(eng_review))\n",
    "print(eng_review.tail(10))\n",
    "\n",
    "# Finally, We add detailed information\n",
    "eng_review['attraction'] = 'attraction'\n",
    "eng_review['address'] = 'address'\n",
    "eng_review['latitude'] = 36.7823029\n",
    "eng_review['longitude'] = 126.4591244\n",
    "#eng_review.to_csv(\"양양/동호해수욕장.csv\", encoding=\"utf-8\", index = None) # 맥북 기준\n",
    "eng_review.to_csv(\"양양/동호해수욕장.csv\", encoding=\"utf-8\", index = None) # Window 기준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Advisor Crawling code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium version setting\n",
    "# !pip install --upgrade selenium==3.141.0\n",
    "\n",
    "# import package\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By  # 3개는 selenium이 동작하면서 웹 화면이 로딩될때 까지 기다림을 지원\n",
    "from selenium.webdriver.support.ui import WebDriverWait # 웹 드라이버 미리 설치\n",
    "from selenium.webdriver.support import expected_conditions as EC # support안에 있는 expected_conditions를 EC로 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in chromedriver\n",
    "driver = webdriver.Chrome('C:/Users/chromedriver.exe')\n",
    "# enter review url\n",
    "url = \"https://www.tripadvisor.com/Attraction_Review-g612375-d3805539-Reviews-Naksansa_Temple-Yangyang_gun_Gangwon_do.html\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "# review item list \n",
    "box_list = []\n",
    "name_list = []\n",
    "stars_list = []\n",
    "review_list = []\n",
    "date_list = []\n",
    "\n",
    "num_page = 5\n",
    "i=0\n",
    "while (i<num_page):\n",
    "    # review box class\n",
    "    boxes = driver.find_elements_by_css_selector('div._c')\n",
    "\n",
    "    for box in boxes:\n",
    "        try:\n",
    "            # more button click\n",
    "            more_btn = box.find_element_by_class_name('biGQs._P.XWJSj.Wb')\n",
    "            more_btn.click()\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # review collection\n",
    "    for box in boxes:\n",
    "        # each class location\n",
    "        IDs = box.find_elements_by_css_selector('span.biGQs._P.fiohW.fOtGX')\n",
    "        Dates = box.find_elements_by_class_name('biGQs._P.pZUbB.ncFvv.osNWb')\n",
    "        Stars = box.find_elements_by_css_selector('svg.UctUV.d.H0')      \n",
    "        reviews = box.find_elements_by_class_name('biGQs._P.pZUbB.KxBGd')\n",
    "\n",
    "        # append to list after each review collection\n",
    "        for ID,Date,Star,review in zip(IDs, Dates, Stars, reviews):\n",
    "            name_list.append(ID.text)\n",
    "            date_list.append(Date.text)\n",
    "            stars_list.append(Star.get_attribute('aria-label'))\n",
    "            review_list.append(review.text)\n",
    "\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        driver.find_element_by_class_name(a.BrOJk.u.j.z._F.bYExr.tIqAi.unMkR).click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "review_data = pd.DataFrame(\n",
    "    {'name': name_list,\n",
    "     'rating': stars_list,\n",
    "     'review': review_list,\n",
    "     'date': date_list})\n",
    "\n",
    "# remove unnecessary words\n",
    "eng_review = review_data.copy()\n",
    "\n",
    "# remove not english sentence & special characters\n",
    "eng_review['review'] = eng_review['review'].str.replace(r'[^A-Za-z0-9]',' ', regex=True)\n",
    "\n",
    "# delete side blank and \"\\n\"\n",
    "eng_review['review'] = eng_review['review'].str.strip()\n",
    "\n",
    "# replace with lowercase\n",
    "eng_review['review'] = eng_review['review'].str.lower()\n",
    "\n",
    "#date에서 Written 없애기\n",
    "eng_review['date'] = eng_review['date'].str.replace('Written ','')\n",
    "\n",
    "print(\"number of review:\", len(eng_review))\n",
    "\n",
    "# Finally, We add detailed information\n",
    "eng_review['attraction'] = 'attraction'\n",
    "eng_review['address'] = 'address'\n",
    "eng_review['latitude'] = 38.1253749\n",
    "eng_review['longitude'] = 128.6307526\n",
    "#eng_review.to_csv(\"양양/의상대_trip.csv\", encoding=\"utf-8\", index = None) # 맥북 기준\n",
    "eng_review.to_csv(\"양양/의상대_trip.csv\", encoding=\"cp949\", index = None) # Window 기준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style= \"color:blue\"> 이후 엑셀로 추가작업하였습니다.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Colab][1]으로 전처리를 수행해 해당 링크 안에서 실행하는 것을 권장합니다.\n",
    "[1]: https://colab.research.google.com/drive/1e8DpXuwMO1WTEBxRKqEhwFvhubNyk05y?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T02:31:04.901984Z",
     "start_time": "2022-08-24T02:30:53.629924Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\YOONSU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\YOONSU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\YOONSU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\YOONSU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\YOONSU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/date17-22_data.csv')\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "#단어 토큰화 전처리 코드\n",
    "text_data = data['review']\n",
    "text_data = text_data.str.replace(r'[^ a-zA-Z]',' ', regex=True) #숫자 제거\n",
    "#[a-zA-Z] #[^A-Za-z0-9] \n",
    "\n",
    "def data_text_cleaning(data):\n",
    "    tokens = data.split() #토큰화\n",
    "    stops = set(stopwords.words('english')) #불용어 제거\n",
    "    no_stops = [word for word in tokens if not word in stops] \n",
    "    return ' '.join(no_stops) #공백으로 구분된 문자열로 결합하여 결과 반환\n",
    "\n",
    "\n",
    "text_list = list()\n",
    "tokens_list = list()\n",
    "for i in range(len(text_data)):\n",
    "    text_list.append(data_text_cleaning(text_data[i]))\n",
    "    tokens_list.append(text_list[i].split())\n",
    "#print(tokens_list) #list 형태\n",
    "data['tokens'] = tokens_list\n",
    "\n",
    "#두글자 제거\n",
    "noun_list = data['tokens']\n",
    "for i,v in enumerate(noun_list):\n",
    "    for j,k in enumerate(v):\n",
    "        if len(k)<3:\n",
    "            del noun_list[i][j]\n",
    "\n",
    "#품사 부착\n",
    "tokens = data['tokens']\n",
    "tokens_pos_list = []\n",
    "for l in tokens:\n",
    "    pt = pos_tag(l)\n",
    "    tokens_pos_list.append(pt)\n",
    "\n",
    "#품사부착 + 원형복원\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#pos_tag가 반환한 품사를 원형복원용 품사로 변환\n",
    "tokens_pos_list2 = []\n",
    "for token_list in tokens_pos_list:\n",
    "    temp_list = []\n",
    "    for token, pos_tag in token_list:\n",
    "        tag = get_wordnet_pos(pos_tag)\n",
    "        if tag!=None: #변환된 품사가 None이 아닌 것들만 list에 담는다.\n",
    "            temp_list.append((token, get_wordnet_pos(pos_tag)))\n",
    "    tokens_pos_list2.append(temp_list)\n",
    "    \n",
    "#원형복원(lemm.lemmatize)\n",
    "lemm = WordNetLemmatizer()\n",
    "token_final = []\n",
    "for token_pos in tokens_pos_list2:\n",
    "    tmp_list = []\n",
    "    for token, tag in token_pos:\n",
    "        tmp_list.append(lemm.lemmatize(token, pos=tag))\n",
    "    token_final.append(tmp_list)\n",
    "\n",
    "data = data.assign(new_tokens = token_final)\n",
    "\n",
    "#new_tokens 빈 리스트를 가진 행 지우기\n",
    "locate = []\n",
    "for i in range(len(data['review'])):\n",
    "    if data['new_tokens'][i] == []:\n",
    "        locate.append(i)\n",
    "print(locate)\n",
    "\n",
    "data = data.drop(locate, axis=0)\n",
    "data.shape #(4996, 11)\n",
    "\n",
    "#index reset\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#토큰 하나인 리뷰 index 제거\n",
    "locate = []\n",
    "for i in range(len(data['review'])):\n",
    "    if len(data['new_tokens'][i]) == 1:\n",
    "        locate.append(i)\n",
    "\n",
    "data = data.drop(locate, axis=0)\n",
    "data.shape #(4485, 11)\n",
    "\n",
    "#index reset\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = data['new_tokens']\n",
    "str = numpy.concatenate(tokens_list).tolist() #전체 리뷰 문장 토큰화\n",
    "c = Counter(str)\n",
    "c2 = c.most_common(100)\n",
    "print(c2)\n",
    "c3 = dict(c2); c3\n",
    "print(len(dict(Counter(str)))) #단어 table\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                      stopwords = stopwords,\n",
    "                      background_color=\"white\",\n",
    "                      min_font_size = 10).generate_from_frequencies(c3)\n",
    "\n",
    "plt.figure(figsize = (10,15), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 단어 제거\n",
    "\n",
    "rm = ['lot', 'bit', 'day', 'area', 'place', 'one', 'view','visit', 'sokcho', 'gangneung', 'spot', 'trip', 'way', 'people', \n",
    "      'thing', 'get', 'part', 'st', 'km', 'pm', 'chi', 'krw', 'thru', 'throll', 'also','good','great']\n",
    "for i,v in enumerate(data['new_tokens']):\n",
    "    for j,k in enumerate(v):\n",
    "        if k in rm:\n",
    "            del data['new_tokens'][i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다시 워드클라우드\n",
    "\n",
    "tokens_list = data['new_tokens']\n",
    "str = numpy.concatenate(tokens_list).tolist() #전체 리뷰 문장 토큰화\n",
    "c = Counter(str)\n",
    "c2 = c.most_common(50)\n",
    "print(c2)\n",
    "c3 = dict(c2); c3\n",
    "len(dict(Counter(str))) #중복되지 않는 단어의 개수\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                      stopwords = stopwords,\n",
    "                      background_color=\"white\",\n",
    "                      min_font_size = 10).generate_from_frequencies(c3)\n",
    "\n",
    "plt.figure(figsize = (10,15), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T02:42:15.247337Z",
     "start_time": "2022-08-24T02:42:15.240271Z"
    }
   },
   "source": [
    "## LDA 토픽모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T02:59:14.710155Z",
     "start_time": "2022-08-24T02:59:14.699659Z"
    }
   },
   "source": [
    "* [Colab][1]으로 LDA 토픽모델링을 수행해 해당 링크 안에서 실행하는 것을 권장합니다.\n",
    "[1]: https://colab.research.google.com/drive/1e8DpXuwMO1WTEBxRKqEhwFvhubNyk05y?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.callbacks import CoherenceMetric\n",
    "from gensim import corpora\n",
    "from gensim.models.callbacks import PerplexityMetric\n",
    "import logging\n",
    "\n",
    "!pip install pyLDAvis\n",
    "import pickle\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data['new_tokens']\n",
    "\n",
    "dictionary = corpora.Dictionary.load(\"/content/dic.csv\")\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5) \n",
    "corpus = [dictionary.doc2bow(text) for text in processed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preplexity 값: 확률 모델이 결과를 얼마나 정확하게 예측하는지/ 낮을수록 정확하게 예측\n",
    "'''chunksize = 1000 \n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "\n",
    "preplexity_values=[]\n",
    "for i in range(2,15):\n",
    "  ldamodel=gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=dictionary, random_state=40,\n",
    "                                           chunksize=chunksize, passes = passes, iterations=iterations, eval_every=eval_every)\n",
    "  preplexity_values.append(ldamodel.log_perplexity(corpus))\n",
    "\n",
    "x=range(2,15)\n",
    "plt.plot(x, preplexity_values)\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"perplexity score\")\n",
    "plt.show() #6~8에서 완만해짐'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherence 값: 토픽이 얼마나 의미론적으로 일관성 있는지/ 높을수록 의미론적 일관성 높음\n",
    "'''coherence_values=[]\n",
    "for i in range(2,15):\n",
    "  ldamodel=gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=dictionary, random_state=40,\n",
    "                                           chunksize=chunksize, passes = passes, iterations=iterations, eval_every=eval_every)\n",
    "  coherence_model_lda=CoherenceModel(model=ldamodel, texts=data['new_tokens'], dictionary=dictionary, topn=10)\n",
    "  coherence_lda=coherence_model_lda.get_coherence()\n",
    "  coherence_values.append(coherence_lda)\n",
    "\n",
    "x=range(2,15)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"coherence score\")\n",
    "plt.show() #topic 6이 최적\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1000 \n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=6, id2word=dictionary, random_state=40,\n",
    "                                           chunksize=chunksize, passes = passes, iterations=iterations, eval_every=eval_every)\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(\"topic 별 단어:\", topic)\n",
    "    \n",
    "x =[]\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    x.append(topic[1])\n",
    "print(\"topic별 단어리스트:\", x)\n",
    "\n",
    "# LDA visualization\n",
    "vis = gensimvis.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'topic6_last.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if i==5:\n",
    "        break\n",
    "    print(i,'번째 문서의 topic 비율은',topic_list)\n",
    "\n",
    "def make_topictable_per_doc(ldamodel, corpus):\n",
    "    topic_table = pd.DataFrame()\n",
    "\n",
    "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
    "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
    "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
    "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
    "\n",
    "        # 모든 문서에 대해서 각각 아래를 수행\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
    "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
    "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)\n",
    "\n",
    "topictable = make_topictable_per_doc(ldamodel, corpus)\n",
    "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
    "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
    "topictable[:10]\n",
    "\n",
    "data['가장 비중이 높은 토픽'] = topictable['가장 비중이 높은 토픽']\n",
    "data.rename(columns={\"가장 비중이 높은 토픽\": \"topic\"}, inplace=True)\n",
    "data.to_csv('topic6_data.csv', index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽별 상위 10개 단어 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fiz=plt.figure(figsize=(30,15))\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "\n",
    "for i in range(6):\n",
    "    df=pd.DataFrame(ldamodel.show_topic(i), columns=['term','prob']).set_index('term')\n",
    "    \n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.rc('xtick', labelsize=15)\n",
    "    plt.rc('ytick', labelsize=15)\n",
    "    c_list = ['lightcoral','mediumseagreen','darkkhaki','darkturquoise','orchid','cornflowerblue']\n",
    "    t_name = ['식도락 관광', '워터투어', '산행하기 좋은 곳', '낭만적인 장소', '한국의 문화체험관광', '에코 힐링']\n",
    "    plt.title('topic {0} : {1}'.format(i+1, t_name[i]),fontweight=\"bold\",size=18)\n",
    "    sns.barplot(x='prob', y=df.index, data=df, color=c_list[i])\n",
    "    plt.xlabel('probability',size=14)\n",
    "    plt.ylabel('term',size=14)\n",
    "\n",
    "plt.savefig('savefig_default.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T04:23:23.331002Z",
     "start_time": "2022-08-24T04:23:23.316988Z"
    }
   },
   "source": [
    "## 명소별 토픽 선정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 명소별 토픽 선정 후 시각화는 [태블로][1]로 작업해 해당 링크에서 확인할 수 있습니다.\n",
    "[1]: https://public.tableau.com/app/profile/.74155837/viz/___16613166394110/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"topic6_data.csv\", encoding='cp949')\n",
    "print(df.head())\n",
    "print(df.topic.value_counts())\n",
    "\n",
    "# 리뷰별 토픽 최빈값으로 명소별 토픽 추출\n",
    "df2 = pd.DataFrame(df.groupby(['attraction','latitude','longitude','address']).topic.apply(lambda x: x.mode()))\n",
    "\n",
    "df2.reset_index(inplace=True)\n",
    "df2.drop('level_4', axis=1, inplace=True)\n",
    "\n",
    "# 최빈값이 두개이상일때 마지막 값으로 표시\n",
    "df2 = df2[df2['attraction'].duplicated(keep='last')==False]\n",
    "\n",
    "df2.to_csv(\"topic6_unique.csv\", encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 입력한 단어가 속한 데이터보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"/root/topic6_data_final.csv\", encoding=\"cp949\")\n",
    "print(df.head())\n",
    "\n",
    "df.word = df.new_tokens.str.findall('expensive') # input the word you want to find\n",
    "print(df[df.word.str.len() != 0].shape)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df_len = len(df[df.word.str.len() != 0])\n",
    "\n",
    "df2 = df[df.word.str.len() != 0][['attraction','review','new_topic']]\n",
    "df2[df2['new_topic']==1] # input the topic you want to find"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "379.387px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
